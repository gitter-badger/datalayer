{"paragraphs":[{"text":"%md HDFS Locality test (taken with big thanks from https://github.com/apache-spark-on-k8s/kubernetes-HDFS)","user":"anonymous","dateUpdated":"2017-11-21T16:27:24+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511281573082_-44356108","id":"20171121-162613_1871875075","dateCreated":"2017-11-21T16:26:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:627","dateFinished":"2017-11-21T16:27:27+0000","dateStarted":"2017-11-21T16:27:24+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>HDFS Locality test (taken with big thanks from <a href=\"https://github.com/apache-spark-on-k8s/kubernetes-HDFS\">https://github.com/apache-spark-on-k8s/kubernetes-HDFS</a>)</p>\n</div>"}]}},{"text":"%sh \nhdfs dfsadmin -safemode leave\nhdfs dfs -cp file:/etc/hosts /hosts\nhdfs dfs -ls /hosts","user":"anonymous","dateUpdated":"2017-11-21T17:01:07+0000","config":{"colWidth":4,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511281498907_1930689292","id":"20171121-162458_1062694110","dateCreated":"2017-11-21T16:24:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:298","dateFinished":"2017-11-21T16:29:16+0000","dateStarted":"2017-11-21T16:29:15+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%sh\nhdfs dfs -setrep 3 /hosts","user":"anonymous","dateUpdated":"2017-11-21T17:01:31+0000","config":{"colWidth":4,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511281531463_77494084","id":"20171121-162531_829934194","dateCreated":"2017-11-21T16:25:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:493","dateFinished":"2017-11-21T16:29:39+0000","dateStarted":"2017-11-21T16:29:38+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Replication 3 set: /hosts\n"}]}},{"text":"%sh\n# Get the local node host IP address\nkubectl get pods $HOSTNAME -o jsonpath=\"{.status.hostIP}\"","user":"anonymous","dateUpdated":"2017-11-21T17:01:15+0000","config":{"colWidth":4,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511281872809_-1868645091","id":"20171121-163112_198984048","dateCreated":"2017-11-21T16:31:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1100","dateFinished":"2017-11-21T16:36:17+0000","dateStarted":"2017-11-21T16:36:17+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"10.0.0.90"}]}},{"text":"%sh\n","user":"anonymous","dateUpdated":"2017-11-21T16:29:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511281797995_1854856340","id":"20171121-162957_561341108","dateCreated":"2017-11-21T16:29:57+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:912"},{"text":"%md \n\nRun the following `hdfs cat` command and from the debug messages and see which datanode is being used.\n\nMake sure it is your local datanode (IP address printed in previous paragraph).\n\n```\n\n$ hadoop --loglevel DEBUG fs  \\\n  -fs hdfs://hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local  \\\n  -cat /hosts\n...\n17/04/24 20:51:28 DEBUG hdfs.DFSClient: Connecting to datanode 10.128.0.4:50010\n...\n```\n\nIf no, you should check if your local datanode is even in the list from the debug messsages above. If it is not, then this is because step (3) did not finish yet. Wait more. (You can use a smaller cluster for this test if that is possible)\n\n```\n17/04/24 20:51:28 DEBUG hdfs.DFSClient: newInfo = LocatedBlocks{\nfileLength=199\n  underConstruction=false\n    blocks=[LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n    getBlockSize()=199; corrupt=false; offset=0;\n    locs=[DatanodeInfoWithStorage[10.128.0.4:50010,DS-d2de9d29-6962-4435-a4b4-aadf4ea67e46,DISK],\n    DatanodeInfoWithStorage[10.128.0.3:50010,DS-0728ffcf-f400-4919-86bf-af0f9af36685,DISK],\n    DatanodeInfoWithStorage[10.128.0.2:50010,DS-3a881114-af08-47de-89cf-37dec051c5c2,DISK]]}]\n      lastLocatedBlock=LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n```\n\nRepeat the `hdfs cat` command multiple times. Check if the same datanode is being consistently used.","user":"anonymous","dateUpdated":"2017-11-21T16:40:54+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511281571277_-112861580","id":"20171121-162611_1992349806","dateCreated":"2017-11-21T16:26:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:560","dateFinished":"2017-11-21T16:39:10+0000","dateStarted":"2017-11-21T16:39:10+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Run the following <code>hdfs cat</code> command and from the debug messages and see which datanode is being used.</p>\n<p>Make sure it is your local datanode (IP address printed in previous paragraph).</p>\n<pre><code><br/>$ hadoop --loglevel DEBUG fs  \\\n  -fs hdfs://hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local  \\\n  -cat /hosts\n...\n17/04/24 20:51:28 DEBUG hdfs.DFSClient: Connecting to datanode 10.128.0.4:50010\n...\n</code></pre>\n<p>If no, you should check if your local datanode is even in the list from the debug messsages above. If it is not, then this is because step (3) did not finish yet. Wait more. (You can use a smaller cluster for this test if that is possible)</p>\n<pre><code>17/04/24 20:51:28 DEBUG hdfs.DFSClient: newInfo = LocatedBlocks{\nfileLength=199\n  underConstruction=false\n    blocks=[LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n    getBlockSize()=199; corrupt=false; offset=0;\n    locs=[DatanodeInfoWithStorage[10.128.0.4:50010,DS-d2de9d29-6962-4435-a4b4-aadf4ea67e46,DISK],\n    DatanodeInfoWithStorage[10.128.0.3:50010,DS-0728ffcf-f400-4919-86bf-af0f9af36685,DISK],\n    DatanodeInfoWithStorage[10.128.0.2:50010,DS-3a881114-af08-47de-89cf-37dec051c5c2,DISK]]}]\n      lastLocatedBlock=LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n</code></pre>\n<p>Repeat the <code>hdfs cat</code> command multiple times. Check if the same datanode is being consistently used.</p>\n</div>"}]}},{"text":"%sh hdfs --loglevel DEBUG dfs -cat /hosts","user":"anonymous","dateUpdated":"2017-11-21T16:59:11+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511281530669_1854217074","id":"20171121-162530_1287902751","dateCreated":"2017-11-21T16:25:30+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:426","dateFinished":"2017-11-21T16:52:47+0000","dateStarted":"2017-11-21T16:52:46+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"17/11/21 16:52:46 DEBUG util.Shell: setsid exited with exit code 0\n17/11/21 16:52:46 DEBUG conf.Configuration: parsing URL jar:file:/opt/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar!/core-default.xml\n17/11/21 16:52:46 DEBUG conf.Configuration: parsing input stream sun.net.www.protocol.jar.JarURLConnection$JarURLInputStream@71423665\n17/11/21 16:52:46 DEBUG conf.Configuration: parsing URL file:/opt/hadoop/etc/hadoop/core-site.xml\n17/11/21 16:52:46 DEBUG conf.Configuration: parsing input stream java.io.BufferedInputStream@fad74ee\n17/11/21 16:52:46 DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])\n17/11/21 16:52:46 DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])\n17/11/21 16:52:46 DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(sampleName=Ops, always=false, about=, type=DEFAULT, valueName=Time, value=[GetGroups])\n17/11/21 16:52:46 DEBUG impl.MetricsSystemImpl: UgiMetrics, User and group related metrics\n17/11/21 16:52:47 DEBUG util.KerberosName: Kerberos krb5 configuration not found, setting default realm to empty\n17/11/21 16:52:47 DEBUG security.Groups:  Creating new Groups object\n17/11/21 16:52:47 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\n17/11/21 16:52:47 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\n17/11/21 16:52:47 DEBUG security.JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution\n17/11/21 16:52:47 DEBUG security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping\n17/11/21 16:52:47 DEBUG security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000\n17/11/21 16:52:47 DEBUG security.UserGroupInformation: hadoop login\n17/11/21 16:52:47 DEBUG security.UserGroupInformation: hadoop login commit\n17/11/21 16:52:47 DEBUG security.UserGroupInformation: using local user:UnixPrincipal: root\n17/11/21 16:52:47 DEBUG security.UserGroupInformation: Using user: \"UnixPrincipal: root\" with name root\n17/11/21 16:52:47 DEBUG security.UserGroupInformation: User entry: \"root\"\n17/11/21 16:52:47 DEBUG security.UserGroupInformation: UGI loginUser:root (auth:SIMPLE)\n17/11/21 16:52:47 TRACE tracing.SpanReceiverHost: No span receiver names found in dfs.client.htrace.spanreceiver.classes.\n17/11/21 16:52:47 DEBUG hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false\n17/11/21 16:52:47 DEBUG hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = false\n17/11/21 16:52:47 DEBUG hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false\n17/11/21 16:52:47 DEBUG hdfs.BlockReaderLocal: dfs.domain.socket.path = \n-cat: java.net.UnknownHostException: hdfs-k8s-hdfs-k8s-hdfs-nn-0.hdfs-k8s-hdfs-k8s-hdfs-nn-0\nUsage: hadoop fs [generic options] -cat [-ignoreCrc] <src> ...\n"},{"type":"TEXT","data":"ExitValue: 255"}]}},{"text":"%sh\n","user":"anonymous","dateUpdated":"2017-11-21T16:30:31+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511281831532_1929418515","id":"20171121-163031_1192038626","dateCreated":"2017-11-21T16:30:31+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1005"}],"name":"HDFS Locality","id":"2D1UAHBVM","angularObjects":{},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}