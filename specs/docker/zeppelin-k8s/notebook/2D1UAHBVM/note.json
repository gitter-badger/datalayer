{"paragraphs":[{"text":"%md # HDFS Locality Test\n\n**As described on https://github.com/apache-spark-on-k8s/kubernetes-HDFS**\n\nPut a local file on HDFS and set replication factor to e.g. 3.\n\n```\nhdfs dfs -cp file:/etc/hosts /hosts\nhdfs dfs -ls /hosts\nhdfs dfs -setrep 3 /hosts\n```\n\nRun the following `hdfs cat` command and from the debug messages, see which datanode is being used and make sure it is your local datanode (use kubectl to get the IP addresses).\n\n```\n$ hdfs --loglevel DEBUG dfs -cat /hosts\n...\n17/04/24 20:51:28 DEBUG hdfs.DFSClient: Connecting to datanode 10.128.0.4:50010\n...\n```\n\nIf not, you should check if your local datanode is even in the list from the debug messsages above. If it is not, then this is because step (3) did not finish yet. Wait more. (You can use a smaller cluster for this test if that is possible)\n\n```\n17/04/24 20:51:28 DEBUG hdfs.DFSClient: newInfo = LocatedBlocks{\nfileLength=199\n  underConstruction=false\n    blocks=[LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n    getBlockSize()=199; corrupt=false; offset=0;\n    locs=[DatanodeInfoWithStorage[10.128.0.4:50010,DS-d2de9d29-6962-4435-a4b4-aadf4ea67e46,DISK],\n    DatanodeInfoWithStorage[10.128.0.3:50010,DS-0728ffcf-f400-4919-86bf-af0f9af36685,DISK],\n    DatanodeInfoWithStorage[10.128.0.2:50010,DS-3a881114-af08-47de-89cf-37dec051c5c2,DISK]]}]\n      lastLocatedBlock=LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n```\n\nRepeat the `hdfs cat` command multiple times. Check if the same datanode is being consistently used.","user":"anonymous","dateUpdated":"2017-11-22T12:53:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>HDFS Locality Test</h1>\n<p>**As described on <a href=\"https://github.com/apache-spark-on-k8s/kubernetes-HDFS**\">https://github.com/apache-spark-on-k8s/kubernetes-HDFS**</a></p>\n<p>Put a local file on HDFS and set replication factor to e.g. 3.</p>\n<pre><code>hdfs dfs -cp file:/etc/hosts /hosts\nhdfs dfs -ls /hosts\nhdfs dfs -setrep 3 /hosts\n</code></pre>\n<p>Run the following <code>hdfs cat</code> command and from the debug messages, see which datanode is being used and make sure it is your local datanode (use kubectl to get the IP addresses).</p>\n<pre><code>$ hdfs --loglevel DEBUG dfs -cat /hosts\n...\n17/04/24 20:51:28 DEBUG hdfs.DFSClient: Connecting to datanode 10.128.0.4:50010\n...\n</code></pre>\n<p>If not, you should check if your local datanode is even in the list from the debug messsages above. If it is not, then this is because step (3) did not finish yet. Wait more. (You can use a smaller cluster for this test if that is possible)</p>\n<pre><code>17/04/24 20:51:28 DEBUG hdfs.DFSClient: newInfo = LocatedBlocks{\nfileLength=199\n  underConstruction=false\n    blocks=[LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n    getBlockSize()=199; corrupt=false; offset=0;\n    locs=[DatanodeInfoWithStorage[10.128.0.4:50010,DS-d2de9d29-6962-4435-a4b4-aadf4ea67e46,DISK],\n    DatanodeInfoWithStorage[10.128.0.3:50010,DS-0728ffcf-f400-4919-86bf-af0f9af36685,DISK],\n    DatanodeInfoWithStorage[10.128.0.2:50010,DS-3a881114-af08-47de-89cf-37dec051c5c2,DISK]]}]\n      lastLocatedBlock=LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n</code></pre>\n<p>Repeat the <code>hdfs cat</code> command multiple times. Check if the same datanode is being consistently used.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511287369656_1797589836","id":"20171121-162613_1871875075","dateCreated":"2017-11-21T18:02:49+0000","dateStarted":"2017-11-22T12:53:43+0000","dateFinished":"2017-11-22T12:53:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:288"},{"text":"%sh \n# hdfs dfsadmin -safemode leave\nhdfs dfs -cp file:/etc/hosts /hosts\nhdfs dfs -ls /hosts\nhdfs dfs -setrep 3 /hosts","user":"anonymous","dateUpdated":"2017-11-22T12:35:19+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"cp: `/hosts': File exists\n-rw-r--r--   3 root supergroup        221 2017-11-22 09:43 /hosts\nReplication 3 set: /hosts\n"}]},"apps":[],"jobName":"paragraph_1511287369659_834467328","id":"20171121-162458_1062694110","dateCreated":"2017-11-21T18:02:49+0000","dateStarted":"2017-11-22T12:35:19+0000","dateFinished":"2017-11-22T12:35:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289"},{"text":"%sh\n# Get the local node host IP address\necho \"Pod name: \"$HOSTNAME\necho \"Host IP: \"$(kubectl get pods $HOSTNAME -o jsonpath=\"{.status.hostIP}\")\necho \"Pod IP: \"$(kubectl get pods $HOSTNAME -o jsonpath=\"{.status.podIP}\")","user":"anonymous","dateUpdated":"2017-11-22T12:35:30+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Pod name: zeppelin-k8s-hdfs-locality-zeppelin-5879cb8d5f-spqk6\nHost IP: 10.0.0.22\nPod IP: 192.168.235.196\n"}]},"apps":[],"jobName":"paragraph_1511287369659_1573570675","id":"20171121-163112_198984048","dateCreated":"2017-11-21T18:02:49+0000","dateStarted":"2017-11-22T12:35:30+0000","dateFinished":"2017-11-22T12:35:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:290"},{"text":"%sh hdfs --loglevel DEBUG dfs -cat /hosts","user":"anonymous","dateUpdated":"2017-11-22T12:39:25+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"17/11/22 12:39:25 DEBUG util.Shell: setsid exited with exit code 0\n17/11/22 12:39:25 DEBUG conf.Configuration: parsing URL jar:file:/opt/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar!/core-default.xml\n17/11/22 12:39:25 DEBUG conf.Configuration: parsing input stream sun.net.www.protocol.jar.JarURLConnection$JarURLInputStream@7e0b0338\n17/11/22 12:39:25 DEBUG conf.Configuration: parsing URL file:/etc/hdfs-k8s-locality/conf/core-site.xml\n17/11/22 12:39:25 DEBUG conf.Configuration: parsing input stream java.io.BufferedInputStream@480bdb19\n17/11/22 12:39:25 DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])\n17/11/22 12:39:25 DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])\n17/11/22 12:39:25 DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[GetGroups])\n17/11/22 12:39:25 DEBUG impl.MetricsSystemImpl: UgiMetrics, User and group related metrics\n17/11/22 12:39:25 DEBUG util.KerberosName: Kerberos krb5 configuration not found, setting default realm to empty\n17/11/22 12:39:25 DEBUG security.Groups:  Creating new Groups object\n17/11/22 12:39:25 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\n17/11/22 12:39:25 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\n17/11/22 12:39:25 DEBUG security.JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution\n17/11/22 12:39:25 DEBUG security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping\n17/11/22 12:39:25 DEBUG security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000\n17/11/22 12:39:25 DEBUG security.UserGroupInformation: hadoop login\n17/11/22 12:39:25 DEBUG security.UserGroupInformation: hadoop login commit\n17/11/22 12:39:25 DEBUG security.UserGroupInformation: using local user:UnixPrincipal: root\n17/11/22 12:39:25 DEBUG security.UserGroupInformation: Using user: \"UnixPrincipal: root\" with name root\n17/11/22 12:39:25 DEBUG security.UserGroupInformation: User entry: \"root\"\n17/11/22 12:39:25 DEBUG security.UserGroupInformation: UGI loginUser:root (auth:SIMPLE)\n17/11/22 12:39:26 DEBUG hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false\n17/11/22 12:39:26 DEBUG hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = false\n17/11/22 12:39:26 DEBUG hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false\n17/11/22 12:39:26 DEBUG hdfs.BlockReaderLocal: dfs.domain.socket.path = \n17/11/22 12:39:26 DEBUG retry.RetryUtils: multipleLinearRandomRetry = null\n17/11/22 12:39:26 DEBUG ipc.Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@229c6181\n17/11/22 12:39:26 DEBUG ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6d4d66d2\n17/11/22 12:39:26 DEBUG unix.DomainSocketWatcher: org.apache.hadoop.net.unix.DomainSocketWatcher$2@7529b0a1: starting with interruptCheckPeriodMs = 60000\n17/11/22 12:39:26 DEBUG util.PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.\n17/11/22 12:39:26 DEBUG sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection\n17/11/22 12:39:26 DEBUG ipc.Client: The ping interval is 60000 ms.\n17/11/22 12:39:26 DEBUG ipc.Client: Connecting to ip-10-0-0-131.us-west-2.compute.internal/10.0.0.131:8020\n17/11/22 12:39:26 DEBUG ipc.Client: IPC Client (2014461570) connection to ip-10-0-0-131.us-west-2.compute.internal/10.0.0.131:8020 from root: starting, having connections 1\n17/11/22 12:39:26 DEBUG ipc.Client: IPC Client (2014461570) connection to ip-10-0-0-131.us-west-2.compute.internal/10.0.0.131:8020 from root sending #0\n17/11/22 12:39:26 DEBUG ipc.Client: IPC Client (2014461570) connection to ip-10-0-0-131.us-west-2.compute.internal/10.0.0.131:8020 from root got value #0\n17/11/22 12:39:26 DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 36ms\n17/11/22 12:39:26 DEBUG ipc.Client: IPC Client (2014461570) connection to ip-10-0-0-131.us-west-2.compute.internal/10.0.0.131:8020 from root sending #1\n17/11/22 12:39:26 DEBUG ipc.Client: IPC Client (2014461570) connection to ip-10-0-0-131.us-west-2.compute.internal/10.0.0.131:8020 from root got value #1\n17/11/22 12:39:26 DEBUG ipc.ProtobufRpcEngine: Call: getBlockLocations took 3ms\n17/11/22 12:39:26 DEBUG hdfs.DFSClient: newInfo = LocatedBlocks{\n  fileLength=221\n  underConstruction=false\n  blocks=[LocatedBlock{BP-2057241701-10.0.0.131-1511343635741:blk_1073741825_1001; getBlockSize()=221; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[10.0.0.22:50010,DS-2efbfa82-c302-491c-8432-6a7f2d9e53f9,DISK], DatanodeInfoWithStorage[10.0.0.90:50010,DS-80984067-3d01-4080-b4f0-2b19772a11dc,DISK]]}]\n  lastLocatedBlock=LocatedBlock{BP-2057241701-10.0.0.131-1511343635741:blk_1073741825_1001; getBlockSize()=221; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[10.0.0.22:50010,DS-2efbfa82-c302-491c-8432-6a7f2d9e53f9,DISK], DatanodeInfoWithStorage[10.0.0.90:50010,DS-80984067-3d01-4080-b4f0-2b19772a11dc,DISK]]}\n  isLastBlockComplete=true}\n17/11/22 12:39:26 DEBUG hdfs.DFSClient: Connecting to datanode ip-10-0-0-22.us-west-2.compute.internal:50010\n17/11/22 12:39:26 DEBUG ipc.Client: IPC Client (2014461570) connection to ip-10-0-0-131.us-west-2.compute.internal/10.0.0.131:8020 from root sending #2\n17/11/22 12:39:26 DEBUG ipc.Client: IPC Client (2014461570) connection to ip-10-0-0-131.us-west-2.compute.internal/10.0.0.131:8020 from root got value #2\n17/11/22 12:39:26 DEBUG ipc.ProtobufRpcEngine: Call: getServerDefaults took 1ms\n17/11/22 12:39:26 DEBUG sasl.SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /10.0.0.22, datanodeId = DatanodeInfoWithStorage[10.0.0.22:50010,DS-2efbfa82-c302-491c-8432-6a7f2d9e53f9,DISK]\n127.0.0.1 localhost\n\n# The following lines are desirable for IPv6 capable hosts\n::1 ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\nff02::3 ip6-allhosts\n17/11/22 12:39:26 DEBUG ipc.Client: stopping client from cache: org.apache.hadoop.ipc.Client@6d4d66d2\n17/11/22 12:39:26 DEBUG ipc.Client: removing client from cache: org.apache.hadoop.ipc.Client@6d4d66d2\n17/11/22 12:39:26 DEBUG ipc.Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@6d4d66d2\n17/11/22 12:39:26 DEBUG ipc.Client: Stopping client\n17/11/22 12:39:26 DEBUG ipc.Client: IPC Client (2014461570) connection to ip-10-0-0-131.us-west-2.compute.internal/10.0.0.131:8020 from root: closed\n17/11/22 12:39:26 DEBUG ipc.Client: IPC Client (2014461570) connection to ip-10-0-0-131.us-west-2.compute.internal/10.0.0.131:8020 from root: stopped, remaining connections 0\n"}]},"apps":[],"jobName":"paragraph_1511287369660_208453910","id":"20171121-162530_1287902751","dateCreated":"2017-11-21T18:02:49+0000","dateStarted":"2017-11-22T12:39:25+0000","dateFinished":"2017-11-22T12:39:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:291"},{"text":"%sh\n","dateUpdated":"2017-11-21T18:02:49+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511287369660_-2138342023","id":"20171121-163031_1192038626","dateCreated":"2017-11-21T18:02:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:292"}],"name":"3. HDFS Locality","id":"2D1UAHBVM","angularObjects":{"2CCMX5R9J:shared_process":[],"2CBEJNFR7:shared_process":[],"2CBRBCK1E:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}