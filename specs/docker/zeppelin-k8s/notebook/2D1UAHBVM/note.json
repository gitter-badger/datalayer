{"paragraphs":[{"text":"%md HDFS Locality test (taken with big thanks from https://github.com/apache-spark-on-k8s/kubernetes-HDFS)","dateUpdated":"2017-11-21T18:02:49+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>HDFS Locality test (taken with big thanks from <a href=\"https://github.com/apache-spark-on-k8s/kubernetes-HDFS\">https://github.com/apache-spark-on-k8s/kubernetes-HDFS</a>)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511287369656_1797589836","id":"20171121-162613_1871875075","dateCreated":"2017-11-21T18:02:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:268"},{"text":"%sh \nhdfs dfsadmin -safemode leave\nhdfs dfs -cp file:/etc/hosts /hosts\nhdfs dfs -ls /hosts\nhdfs dfs -setrep 3 /hosts","dateUpdated":"2017-11-21T18:11:12+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Safe mode is OFF\ncp: `/hosts': File exists\n-rw-r--r--   3 root supergroup        238 2017-11-21 18:03 /hosts\nReplication 3 set: /hosts\n"}]},"apps":[],"jobName":"paragraph_1511287369659_834467328","id":"20171121-162458_1062694110","dateCreated":"2017-11-21T18:02:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:269","user":"anonymous","dateFinished":"2017-11-21T18:11:19+0000","dateStarted":"2017-11-21T18:11:12+0000"},{"text":"%sh\n# Get the local node host IP address\necho \"Pod name: \"$HOSTNAME\necho \"Host IP: \"$(kubectl get pods $HOSTNAME -o jsonpath=\"{.status.hostIP}\")\necho \"Pod IP: \"$(kubectl get pods $HOSTNAME -o jsonpath=\"{.status.podIP}\")","dateUpdated":"2017-11-21T18:11:51+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Pod name: zeppelin-k8s-zeppelin-6f6cc7c4d5-zwfb2\nHost IP: 10.0.0.22\nPod IP: 192.168.235.243\n"}]},"apps":[],"jobName":"paragraph_1511287369659_1573570675","id":"20171121-163112_198984048","dateCreated":"2017-11-21T18:02:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:271","user":"anonymous","dateFinished":"2017-11-21T18:11:51+0000","dateStarted":"2017-11-21T18:11:51+0000"},{"text":"%sh\n","dateUpdated":"2017-11-21T18:02:49+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511287369659_1009633391","id":"20171121-162957_561341108","dateCreated":"2017-11-21T18:02:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:272"},{"text":"%md \n\nRun the following `hdfs cat` command and from the debug messages and see which datanode is being used.\n\nMake sure it is your local datanode (IP address printed in previous paragraph).\n\n```\n\n$ hadoop --loglevel DEBUG fs  \\\n  -fs hdfs://hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local  \\\n  -cat /hosts\n...\n17/04/24 20:51:28 DEBUG hdfs.DFSClient: Connecting to datanode 10.128.0.4:50010\n...\n```\n\nIf no, you should check if your local datanode is even in the list from the debug messsages above. If it is not, then this is because step (3) did not finish yet. Wait more. (You can use a smaller cluster for this test if that is possible)\n\n```\n17/04/24 20:51:28 DEBUG hdfs.DFSClient: newInfo = LocatedBlocks{\nfileLength=199\n  underConstruction=false\n    blocks=[LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n    getBlockSize()=199; corrupt=false; offset=0;\n    locs=[DatanodeInfoWithStorage[10.128.0.4:50010,DS-d2de9d29-6962-4435-a4b4-aadf4ea67e46,DISK],\n    DatanodeInfoWithStorage[10.128.0.3:50010,DS-0728ffcf-f400-4919-86bf-af0f9af36685,DISK],\n    DatanodeInfoWithStorage[10.128.0.2:50010,DS-3a881114-af08-47de-89cf-37dec051c5c2,DISK]]}]\n      lastLocatedBlock=LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n```\n\nRepeat the `hdfs cat` command multiple times. Check if the same datanode is being consistently used.","dateUpdated":"2017-11-21T18:02:49+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Run the following <code>hdfs cat</code> command and from the debug messages and see which datanode is being used.</p>\n<p>Make sure it is your local datanode (IP address printed in previous paragraph).</p>\n<pre><code><br/>$ hadoop --loglevel DEBUG fs  \\\n  -fs hdfs://hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local  \\\n  -cat /hosts\n...\n17/04/24 20:51:28 DEBUG hdfs.DFSClient: Connecting to datanode 10.128.0.4:50010\n...\n</code></pre>\n<p>If no, you should check if your local datanode is even in the list from the debug messsages above. If it is not, then this is because step (3) did not finish yet. Wait more. (You can use a smaller cluster for this test if that is possible)</p>\n<pre><code>17/04/24 20:51:28 DEBUG hdfs.DFSClient: newInfo = LocatedBlocks{\nfileLength=199\n  underConstruction=false\n    blocks=[LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n    getBlockSize()=199; corrupt=false; offset=0;\n    locs=[DatanodeInfoWithStorage[10.128.0.4:50010,DS-d2de9d29-6962-4435-a4b4-aadf4ea67e46,DISK],\n    DatanodeInfoWithStorage[10.128.0.3:50010,DS-0728ffcf-f400-4919-86bf-af0f9af36685,DISK],\n    DatanodeInfoWithStorage[10.128.0.2:50010,DS-3a881114-af08-47de-89cf-37dec051c5c2,DISK]]}]\n      lastLocatedBlock=LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n</code></pre>\n<p>Repeat the <code>hdfs cat</code> command multiple times. Check if the same datanode is being consistently used.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511287369659_1387110023","id":"20171121-162611_1992349806","dateCreated":"2017-11-21T18:02:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:273"},{"text":"%sh hdfs --loglevel DEBUG dfs -cat /hosts","dateUpdated":"2017-11-21T18:11:57+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"17/11/21 18:11:57 DEBUG util.Shell: setsid exited with exit code 0\n17/11/21 18:11:57 DEBUG conf.Configuration: parsing URL jar:file:/opt/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar!/core-default.xml\n17/11/21 18:11:57 DEBUG conf.Configuration: parsing input stream sun.net.www.protocol.jar.JarURLConnection$JarURLInputStream@7e0b0338\n17/11/21 18:11:57 DEBUG conf.Configuration: parsing URL file:/etc/hdfs-k8s/conf/core-site.xml\n17/11/21 18:11:57 DEBUG conf.Configuration: parsing input stream java.io.BufferedInputStream@480bdb19\n17/11/21 18:11:57 DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])\n17/11/21 18:11:57 DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])\n17/11/21 18:11:57 DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, sampleName=Ops, always=false, type=DEFAULT, valueName=Time, value=[GetGroups])\n17/11/21 18:11:57 DEBUG impl.MetricsSystemImpl: UgiMetrics, User and group related metrics\n17/11/21 18:11:57 DEBUG util.KerberosName: Kerberos krb5 configuration not found, setting default realm to empty\n17/11/21 18:11:57 DEBUG security.Groups:  Creating new Groups object\n17/11/21 18:11:57 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\n17/11/21 18:11:57 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\n17/11/21 18:11:57 DEBUG security.JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution\n17/11/21 18:11:57 DEBUG security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping\n17/11/21 18:11:57 DEBUG security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000\n17/11/21 18:11:57 DEBUG security.UserGroupInformation: hadoop login\n17/11/21 18:11:57 DEBUG security.UserGroupInformation: hadoop login commit\n17/11/21 18:11:57 DEBUG security.UserGroupInformation: using local user:UnixPrincipal: root\n17/11/21 18:11:57 DEBUG security.UserGroupInformation: Using user: \"UnixPrincipal: root\" with name root\n17/11/21 18:11:57 DEBUG security.UserGroupInformation: User entry: \"root\"\n17/11/21 18:11:57 DEBUG security.UserGroupInformation: UGI loginUser:root (auth:SIMPLE)\n17/11/21 18:11:58 DEBUG hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false\n17/11/21 18:11:58 DEBUG hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = false\n17/11/21 18:11:58 DEBUG hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false\n17/11/21 18:11:58 DEBUG hdfs.BlockReaderLocal: dfs.domain.socket.path = \n17/11/21 18:11:58 DEBUG retry.RetryUtils: multipleLinearRandomRetry = null\n17/11/21 18:11:58 DEBUG ipc.Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@229c6181\n17/11/21 18:11:58 DEBUG ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6d4d66d2\n17/11/21 18:11:58 DEBUG unix.DomainSocketWatcher: org.apache.hadoop.net.unix.DomainSocketWatcher$2@ff4c3b0: starting with interruptCheckPeriodMs = 60000\n17/11/21 18:11:58 DEBUG util.PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.\n17/11/21 18:11:58 DEBUG sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection\n17/11/21 18:11:58 DEBUG ipc.Client: The ping interval is 60000 ms.\n17/11/21 18:11:58 DEBUG ipc.Client: Connecting to hdfs-k8s-hdfs-k8s-hdfs-nn/192.168.9.236:9000\n17/11/21 18:11:58 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-k8s-hdfs-k8s-hdfs-nn/192.168.9.236:9000 from root: starting, having connections 1\n17/11/21 18:11:58 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-k8s-hdfs-k8s-hdfs-nn/192.168.9.236:9000 from root sending #0\n17/11/21 18:11:58 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-k8s-hdfs-k8s-hdfs-nn/192.168.9.236:9000 from root got value #0\n17/11/21 18:11:58 DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 36ms\n17/11/21 18:11:58 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-k8s-hdfs-k8s-hdfs-nn/192.168.9.236:9000 from root sending #1\n17/11/21 18:11:58 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-k8s-hdfs-k8s-hdfs-nn/192.168.9.236:9000 from root got value #1\n17/11/21 18:11:58 DEBUG ipc.ProtobufRpcEngine: Call: getBlockLocations took 2ms\n17/11/21 18:11:58 DEBUG hdfs.DFSClient: newInfo = LocatedBlocks{\n  fileLength=238\n  underConstruction=false\n  blocks=[LocatedBlock{BP-1233555223-192.168.9.236-1511287154012:blk_1073741825_1001; getBlockSize()=238; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.9.237:50010,DS-f48a69aa-b928-4d37-8024-63fd29776012,DISK]]}]\n  lastLocatedBlock=LocatedBlock{BP-1233555223-192.168.9.236-1511287154012:blk_1073741825_1001; getBlockSize()=238; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[192.168.9.237:50010,DS-f48a69aa-b928-4d37-8024-63fd29776012,DISK]]}\n  isLastBlockComplete=true}\n17/11/21 18:11:58 DEBUG hdfs.DFSClient: Connecting to datanode 192.168.9.237:50010\n17/11/21 18:11:58 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-k8s-hdfs-k8s-hdfs-nn/192.168.9.236:9000 from root sending #2\n17/11/21 18:11:58 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-k8s-hdfs-k8s-hdfs-nn/192.168.9.236:9000 from root got value #2\n17/11/21 18:11:58 DEBUG ipc.ProtobufRpcEngine: Call: getServerDefaults took 1ms\n17/11/21 18:11:58 DEBUG sasl.SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /192.168.9.237, datanodeId = DatanodeInfoWithStorage[192.168.9.237:50010,DS-f48a69aa-b928-4d37-8024-63fd29776012,DISK]\n# Kubernetes-managed hosts file.\n127.0.0.1\tlocalhost\n::1\tlocalhost ip6-localhost ip6-loopback\nfe00::0\tip6-localnet\nfe00::0\tip6-mcastprefix\nfe00::1\tip6-allnodes\nfe00::2\tip6-allrouters\n192.168.235.243\tzeppelin-k8s-zeppelin-6f6cc7c4d5-zwfb2\n17/11/21 18:11:58 DEBUG ipc.Client: stopping client from cache: org.apache.hadoop.ipc.Client@6d4d66d2\n17/11/21 18:11:58 DEBUG ipc.Client: removing client from cache: org.apache.hadoop.ipc.Client@6d4d66d2\n17/11/21 18:11:58 DEBUG ipc.Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@6d4d66d2\n17/11/21 18:11:58 DEBUG ipc.Client: Stopping client\n17/11/21 18:11:58 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-k8s-hdfs-k8s-hdfs-nn/192.168.9.236:9000 from root: closed\n17/11/21 18:11:58 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-k8s-hdfs-k8s-hdfs-nn/192.168.9.236:9000 from root: stopped, remaining connections 0\n"}]},"apps":[],"jobName":"paragraph_1511287369660_208453910","id":"20171121-162530_1287902751","dateCreated":"2017-11-21T18:02:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:274","user":"anonymous","dateFinished":"2017-11-21T18:11:59+0000","dateStarted":"2017-11-21T18:11:57+0000"},{"text":"%sh\n","dateUpdated":"2017-11-21T18:02:49+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511287369660_-2138342023","id":"20171121-163031_1192038626","dateCreated":"2017-11-21T18:02:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:275"}],"name":"HDFS Locality","id":"2D1UAHBVM","angularObjects":{"2CBEJNFR7:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}